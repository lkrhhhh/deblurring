import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numbers
from einops import rearrange

from .layers import *
def conv(in_channels, out_channels, kernel_size, bias=False, stride = 1):
    return nn.Conv2d(
        in_channels, out_channels, kernel_size,
        padding=(kernel_size//2), bias=bias, stride = stride)
class SAM(nn.Module):
    def __init__(self, n_feat, kernel_size=1, bias=False):
        super(SAM, self).__init__()
        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)
        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)
        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)

    def forward(self, x, x_img):
        x1 = self.conv1(x)
        img = self.conv2(x) + x_img
        x2 = torch.sigmoid(self.conv3(img))
        x1 = x1*x2
        x1 = x1+x
        return x1

def to_3d(x):
    return rearrange(x, 'b c h w -> b (h w) c')


def to_4d(x, h, w):
    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)


class BiasFree_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(BiasFree_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)

        assert len(normalized_shape) == 1

        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return x / torch.sqrt(sigma + 1e-5) * self.weight


class WithBias_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(WithBias_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)

        assert len(normalized_shape) == 1

        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias


class LayerNorm(nn.Module):
    def __init__(self, dim, LayerNorm_type):
        super(LayerNorm, self).__init__()
        if LayerNorm_type == 'BiasFree':
            self.body = BiasFree_LayerNorm(dim)
        else:
            self.body = WithBias_LayerNorm(dim)

    def forward(self, x):
        h, w = x.shape[-2:]
        return to_4d(self.body(to_3d(x)), h, w)


class DFFN(nn.Module):
    def __init__(self, dim, ffn_expansion_factor, bias):

        super(DFFN, self).__init__()

        hidden_features = int(dim * ffn_expansion_factor)

        self.patch_size = 4

        self.dim = dim
        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)

        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1,
                                groups=hidden_features * 2, bias=bias)

        self.fft = nn.Parameter(torch.ones((hidden_features * 2, 1, 1, self.patch_size, self.patch_size // 2 + 1)))
        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)

    def forward(self, x):
        x = self.project_in(x)
        x_patch = rearrange(x, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,
                            patch2=self.patch_size)
        x_patch_fft = torch.fft.rfft2(x_patch.float())
        x_patch_fft = x_patch_fft * self.fft
        x_patch = torch.fft.irfft2(x_patch_fft, s=(self.patch_size, self.patch_size))
        x = rearrange(x_patch, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,
                      patch2=self.patch_size)
        x1, x2 = self.dwconv(x).chunk(2, dim=1)

        x = F.gelu(x1) * x2
        x = self.project_out(x)
        return x


class FSAS(nn.Module):
    def __init__(self, dim, bias):
        super(FSAS, self).__init__()

        self.to_hidden = nn.Conv2d(dim, dim * 6, kernel_size=1, bias=bias)
        self.to_hidden_dw = nn.Conv2d(dim * 6, dim * 6, kernel_size=3, stride=1, padding=1, groups=dim * 6, bias=bias)

        self.project_out = nn.Conv2d(dim * 2, dim, kernel_size=1, bias=bias)

        self.norm = LayerNorm(dim * 2, LayerNorm_type='WithBias')

        self.patch_size = 4

    def forward(self, x):
        hidden = self.to_hidden(x)

        q, k, v = self.to_hidden_dw(hidden).chunk(3, dim=1)

        q_patch = rearrange(q, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,
                            patch2=self.patch_size)
        k_patch = rearrange(k, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,
                            patch2=self.patch_size)
        q_fft = torch.fft.rfft2(q_patch.float())
        k_fft = torch.fft.rfft2(k_patch.float())

        out = q_fft * k_fft
        out = torch.fft.irfft2(out, s=(self.patch_size, self.patch_size))
        out = rearrange(out, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,
                        patch2=self.patch_size)

        out = self.norm(out)

        output = v * out
        output = self.project_out(output)

        return output


##########################################################################
class TransformerBlock(nn.Module):
    def __init__(self, dim, ffn_expansion_factor=3, bias=False, LayerNorm_type='WithBias', att=False):
        super(TransformerBlock, self).__init__()

        self.att = att
        if self.att:
            self.norm1 = LayerNorm(dim, LayerNorm_type)
            self.attn = FSAS(dim, bias)

        self.norm2 = LayerNorm(dim, LayerNorm_type)
        self.ffn = DFFN(dim, ffn_expansion_factor, bias)

    def forward(self, x):
        if self.att:
            x = x + self.attn(self.norm1(x))

        x = x + self.ffn(self.norm2(x))

        return x


class EBlock(nn.Module):
    def  __init__(self, out_channel, num_res=8):
        super(EBlock, self).__init__()
        #layers = [MABlock(out_channel, out_channel) for _ in range(num_res)]
        layers = [ResBlock(out_channel, out_channel) for _ in range(num_res)]

        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)


class DBlock(nn.Module):
    def __init__(self, channel, num_res=8):
        super(DBlock, self).__init__()
        #layers = [MABlock(channel, channel) for _ in range(num_res)]
        layers = [ResBlock(channel, channel) for _ in range(num_res)]
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)


class AFF(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(AFF, self).__init__()
        self.conv = nn.Sequential(
            BasicConv(in_channel, out_channel, kernel_size=1, stride=1, relu=True),
            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)
        )

    def forward(self, x1, x2, x4):
        x = torch.cat([x1, x2, x4], dim=1)
        return self.conv(x)


class SCM(nn.Module):
    def __init__(self, out_plane):
        super(SCM, self).__init__()
        self.main = nn.Sequential(
            BasicConv(3, out_plane//4, kernel_size=3, stride=1, relu=True),
            BasicConv(out_plane // 4, out_plane // 2, kernel_size=1, stride=1, relu=True),
            BasicConv(out_plane // 2, out_plane // 2, kernel_size=3, stride=1, relu=True),
            BasicConv(out_plane // 2, out_plane-3, kernel_size=1, stride=1, relu=True)
        )

        self.conv = BasicConv(out_plane, out_plane, kernel_size=1, stride=1, relu=False)

    def forward(self, x):
        x = torch.cat([x, self.main(x)], dim=1)
        return self.conv(x)


class FAM(nn.Module):
    def __init__(self, channel):
        super(FAM, self).__init__()
        self.merge = BasicConv(channel, channel, kernel_size=3, stride=1, relu=False)

    def forward(self, x1, x2):
        x = x1 * x2
        out = x1 + self.merge(x)
        return out

class SCA(nn.Module):
    def __init__(self):
        super(SCA, self).__init__()
        self.one = nn.AdaptiveAvgPool2d(1)

    def forward(self,x):
        x1 = self.one(x)
        x1 = x1.expand_as(x)
        return x1*x





class MIMOUNet(nn.Module):
    def __init__(self,dim = 32,ffn_expansion_factor = 3, bias= False,num_res=8,num_blocks=[6, 6, 12]):
        super(MIMOUNet, self).__init__()

        base_channel = 32

        self.sac1 = SAM(dim)
        self.sac2 = SAM(dim*2)
        self.sac3 = SAM(dim*4)

        self.encoder_level1 = nn.Sequential(
                TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                EBlock(dim, num_res)
            )


        self.encoder_level2 = nn.Sequential(
                TransformerBlock(dim=int(dim * 2), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                TransformerBlock(dim=int(dim * 2), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                TransformerBlock(dim=int(dim * 2), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res)
            )


        self.encoder_level3 = nn.Sequential(
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias),
                EBlock(dim * 4, num_res)
            )

        self.decoder_level3 = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 4, num_res),
                TransformerBlock(dim=int(dim * 4), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 4, num_res)
            )



        self.decoder_level2 = nn.Sequential(
                TransformerBlock(dim=int(dim * 2), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                TransformerBlock(dim=int(dim * 2), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                TransformerBlock(dim=int(dim * 2), ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res),
                EBlock(dim * 2, num_res)
            )


        self.decoder_level1 = nn.Sequential(*[
                TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                TransformerBlock(dim=dim, ffn_expansion_factor=ffn_expansion_factor, bias=bias, att=True),
                EBlock(dim, num_res),
                EBlock(dim, num_res),
                EBlock(dim, num_res)

        self.Encoder = nn.ModuleList([
            EBlock(base_channel, num_res),
            EBlock(base_channel*2, num_res),
            EBlock(base_channel*4, num_res),
        ])


        self.feat_extract = nn.ModuleList([
            BasicConv(3, base_channel, kernel_size=3, relu=True, stride=1),
            BasicConv(base_channel, base_channel*2, kernel_size=3, relu=True, stride=2),
            BasicConv(base_channel*2, base_channel*4, kernel_size=3, relu=True, stride=2),
            BasicConv(base_channel*4, base_channel*2, kernel_size=4, relu=True, stride=2, transpose=True),
            BasicConv(base_channel*2, base_channel, kernel_size=4, relu=True, stride=2, transpose=True),
            BasicConv(base_channel, 3, kernel_size=3, relu=False, stride=1)
        ])

        self.Decoder = nn.ModuleList([
            DBlock(base_channel * 4, num_res),
            DBlock(base_channel * 2, num_res),
            DBlock(base_channel, num_res)
        ])

        self.Convs = nn.ModuleList([
            BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1),
            BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1),
        ])

        self.ConvsOut = nn.ModuleList(
            [
                BasicConv(base_channel * 4, 3, kernel_size=3, relu=False, stride=1),
                BasicConv(base_channel * 2, 3, kernel_size=3, relu=False, stride=1),
            ]
        )

        self.AFFs = nn.ModuleList([
            AFF(base_channel * 7, base_channel*1),
            AFF(base_channel * 7, base_channel*2)
        ])

        self.FAM1 = FAM(base_channel * 4)
        self.SCM1 = SCM(base_channel * 4)
        self.FAM2 = FAM(base_channel * 2)
        self.SCM2 = SCM(base_channel * 2)


        self.drop1 = nn.Dropout2d(0.1)
        self.drop2 = nn.Dropout2d(0.1)

    def forward(self, x):
        x_2 = F.interpolate(x, scale_factor=0.5)
        x_4 = F.interpolate(x_2, scale_factor=0.5)



        z2 = self.SCM2(x_2)
        z4 = self.SCM1(x_4)
        outputs = list()

        x_ = self.feat_extract[0](x)
        res1 = self.encoder_level1(x_)
        res1 = self.sac1(res1,x)

        z = self.feat_extract[1](res1)
        z = self.FAM2(z, z2)
        res2 = self.encoder_level2(z)
        res2 = self.sac2(res2, x_2)

        z = self.feat_extract[2](res2)
        z = self.FAM1(z, z4)
        z = self.encoder_level3(z)
        z = self.sac3(z, x_4)

        z12 = F.interpolate(res1, scale_factor=0.5)
        z21 = F.interpolate(res2, scale_factor=2)
        z42 = F.interpolate(z, scale_factor=2)
        z41 = F.interpolate(z42, scale_factor=2)

        res2 = self.AFFs[1](z12, res2, z42)
        res1 = self.AFFs[0](res1, z21, z41)

        res2 = self.drop2(res2)
        res1 = self.drop1(res1)

        z = self.decoder_level3(z)
        z = self.sac3(z, x_4)
        z_ = self.ConvsOut[0](z)
        z = self.feat_extract[3](z)
        outputs.append(z_ + x_4)

        z = torch.cat([z, res2], dim=1)
        z = self.Convs[0](z)
        z = self.decoder_level2(z)
        z = self.sac2(z, x_2)
        z_ = self.ConvsOut[1](z)
        z = self.feat_extract[4](z)
        outputs.append(z_ + x_2)

        z = torch.cat([z, res1], dim=1)
        z = self.Convs[1](z)
        z = self.decoder_level1(z)
        z = self.sac1(z, x)
        z = self.feat_extract[5](z)
        outputs.append(z + x)

        return outputs

class MIMOUNetPlus(nn.Module):
    def __init__(self, num_res = 20):
        super(MIMOUNetPlus, self).__init__()
        base_channel = 32
        self.Encoder = nn.ModuleList([
            EBlock(base_channel, num_res),
            EBlock(base_channel*2, num_res),
            EBlock(base_channel*4, num_res),
        ])

        self.feat_extract = nn.ModuleList([
            BasicConv(3, base_channel, kernel_size=3, relu=True, stride=1),
            BasicConv(base_channel, base_channel*2, kernel_size=3, relu=True, stride=2),
            BasicConv(base_channel*2, base_channel*4, kernel_size=3, relu=True, stride=2),
            BasicConv(base_channel*4, base_channel*2, kernel_size=4, relu=True, stride=2, transpose=True),
            BasicConv(base_channel*2, base_channel, kernel_size=4, relu=True, stride=2, transpose=True),
            BasicConv(base_channel, 3, kernel_size=3, relu=False, stride=1)
        ])

        self.Decoder = nn.ModuleList([
            DBlock(base_channel * 4, num_res),
            DBlock(base_channel * 2, num_res),
            DBlock(base_channel, num_res)
        ])

        self.Convs = nn.ModuleList([
            BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1),
            BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1),
        ])

        self.ConvsOut = nn.ModuleList(
            [
                BasicConv(base_channel * 4, 3, kernel_size=3, relu=False, stride=1),
                BasicConv(base_channel * 2, 3, kernel_size=3, relu=False, stride=1),
            ]
        )

        self.AFFs = nn.ModuleList([
            AFF(base_channel * 7, base_channel*1),
            AFF(base_channel * 7, base_channel*2)
        ])

        self.FAM1 = FAM(base_channel * 4)
        self.SCM1 = SCM(base_channel * 4)
        self.FAM2 = FAM(base_channel * 2)
        self.SCM2 = SCM(base_channel * 2)

        self.drop1 = nn.Dropout2d(0.1)
        self.drop2 = nn.Dropout2d(0.1)

    def forward(self, x):
        x_2 = F.interpolate(x, scale_factor=0.5)
        x_4 = F.interpolate(x_2, scale_factor=0.5)
        z2 = self.SCM2(x_2)
        z4 = self.SCM1(x_4)

        outputs = list()

        x_ = self.feat_extract[0](x)
        res1 = self.Encoder[0](x_)

        z = self.feat_extract[1](res1)
        z = self.FAM2(z, z2)
        res2 = self.Encoder[1](z)

        z = self.feat_extract[2](res2)
        z = self.FAM1(z, z4)
        z = self.Encoder[2](z)

        z12 = F.interpolate(res1, scale_factor=0.5)
        z21 = F.interpolate(res2, scale_factor=2)
        z42 = F.interpolate(z, scale_factor=2)
        z41 = F.interpolate(z42, scale_factor=2)

        res2 = self.AFFs[1](z12, res2, z42)
        res1 = self.AFFs[0](res1, z21, z41)

        res2 = self.drop2(res2)
        res1 = self.drop1(res1)

        z = self.Decoder[0](z)
        z_ = self.ConvsOut[0](z)
        z = self.feat_extract[3](z)
        outputs.append(z_+x_4)

        z = torch.cat([z, res2], dim=1)
        z = self.Convs[0](z)
        z = self.Decoder[1](z)
        z_ = self.ConvsOut[1](z)
        z = self.feat_extract[4](z)
        outputs.append(z_+x_2)

        z = torch.cat([z, res1], dim=1)
        z = self.Convs[1](z)
        z = self.Decoder[2](z)
        z = self.feat_extract[5](z)
        outputs.append(z+x)

        return outputs


def build_net(model_name):
    class ModelError(Exception):
        def __init__(self, msg):
            self.msg = msg

        def __str__(self):
            return self.msg

    if model_name == "MIMO-UNetPlus":
        return MIMOUNetPlus()
    elif model_name == "MIMO-UNet":
        return MIMOUNet()
    raise ModelError('Wrong Model!\nYou should choose MIMO-UNetPlus or MIMO-UNet.')